{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate noisy datasets\n",
    "Please make sure that datasets are downloaded and properly placed in `./data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise_set(path, name = \"metadata\"):\n",
    "    for noise in [0.0, 0.02, 0.04, 0.06, 0.08, 0.10]:\n",
    "        csv = pd.read_csv(os.path.join(path, f\"{name}.csv\"))\n",
    "        print(csv.columns)\n",
    "        csv['gth'] = csv['y']\n",
    "        for split in range(3):\n",
    "            split_len = csv.loc[(csv[\"split\"] == split)].shape[0]\n",
    "            print(\"#### split = {}, len = {} ####\".format(split, split_len))\n",
    "            sampled_idxs = []\n",
    "            for y in range(2):\n",
    "                subdf = csv.loc[(csv[\"split\"] == split) & (csv[\"y\"] == y)]\n",
    "                group_dfs = [subdf.loc[subdf[\"place\"] == i] for i in range(2)]\n",
    "                n_sample = round(subdf.shape[0] * noise)\n",
    "                if split == 0:\n",
    "                    n_flips =  [n_sample // 2, (n_sample + 1) // 2]  # num of data to be flip in each group of this label\n",
    "                else:\n",
    "                    place_rate = group_dfs[0].shape[0] / subdf.shape[0]\n",
    "                    n_flips = [int(n_sample * place_rate), n_sample - int(n_sample * place_rate)]        \n",
    "\n",
    "                print(\"n_sample: {}, Num of data {}\".format(n_flips, [w.shape[0] for w in group_dfs]))\n",
    "                assert np.all([n <= w.shape[0] for n, w in zip(n_flips, group_dfs)])\n",
    "                sampled_idxs += [np.random.choice(\n",
    "                    w.index.to_numpy(), size = n, replace = False) for n, w in zip(n_flips, group_dfs)]\n",
    "\n",
    "            sampled_idxs = np.concatenate(sampled_idxs)    \n",
    "            csv.loc[sampled_idxs, \"y\"] = csv.loc[sampled_idxs, \"y\"].map(lambda x: 1 - x)\n",
    "            y_noise = csv.loc[(csv[\"split\"] == split) & (csv[\"gth\"] != csv[\"y\"])].shape[0] / split_len * 100\n",
    "            p_noise = csv.loc[(csv[\"split\"] == split) & (csv[\"place\"] != csv[\"y\"])].shape[0] / split_len * 100\n",
    "            print(f\"split = {split}, core noise = {y_noise:.2f}%, spurious noise = {p_noise:.2f}%\")\n",
    "\n",
    "        core_noise = int(100 * noise)\n",
    "        csv.to_csv(os.path.join(path, f\"metadata_{core_noise}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Waterbirds dataset, please download the dataset to `./data/waterbirds_v1.0/` with `metadata.csv` inside.\n",
    "Run the following cell to obtain Waterbirds with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/waterbirds_v1.0\"\n",
    "gen_noise_set(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CelebA dataset, please download the dataset to `./data/celebA_v1.0/` with `metadata.csv` inside.\n",
    "Run the following cell to obtain CelebA with noise. \n",
    "We first subsample the dataset to make noise level consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"./data/celebA_v1.0/metadata.csv\")\n",
    "print(\"Sub-sampling data ...\")\n",
    "lis = []\n",
    "for split in range(3):\n",
    "    df = csv.loc[csv[\"split\"] == split]\n",
    "    bc = df.groupby([\"y\", \"place\"]).agg({\"split\": \"count\"}).reset_index()[\"split\"].to_numpy()\n",
    "    g1_num = int(bc[3] / bc[2] * bc[1])\n",
    "    print(\"original group 1 num is {}, project to {}\".format(bc[0], g1_num))\n",
    "    assert g1_num <= bc[0]\n",
    "    index = df.loc[(df[\"y\"] == 0) & (df[\"place\"] == 0)].index\n",
    "    lis.append(df.drop(index[g1_num:]))\n",
    "    print(lis[-1].groupby([\"y\", \"place\"]).agg({\"split\": \"count\"}).reset_index()[\"split\"].to_numpy())\n",
    "df = pd.concat(lis, ignore_index=True)\n",
    "df.to_csv(\"./data/celebA_v1.0/metadata_subsample.csv\", index = False)\n",
    "\n",
    "path = \"./data/celebA_v1.0\"\n",
    "gen_noise_set(path, \"metadata_subsample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('LMnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad90c906a2771459b5b3c029d4e6adaf2bd8b595ee77944ea746ba8b4d82f429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
